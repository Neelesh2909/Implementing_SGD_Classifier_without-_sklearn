{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H"
      },
      "source": [
        "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv"
      },
      "source": [
        "# please don't change random_state\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79e0f74-0690-4f71-f6b5-1f02563edbb2"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP"
      },
      "source": [
        "#please don't change random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONY1YiDq7jD"
      },
      "source": [
        "# Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(X_train)\n",
        "x_test = scaler.transform(X_test)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02b698f-1ca9-4584-84bf-a3204dad4da2"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cac76b-2d6b-4a44-d4b1-d2bbd8147302"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cba74d-1bc7-4662-a99b-535ba2d2ea07"
      },
      "source": [
        "clf.fit(X=X_train, y=y_train) # fitting our model"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
            "Total training time: 0.09 seconds.\n",
            "Convergence after 10 epochs took 0.09 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbd7e0b-95e1-4fc7-dad3-35615193fc52"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
              "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
              "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
              " (1, 15),\n",
              " array([-0.8531383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_8bdzitDlM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  We will be giving you some functions, please write code in that functions only.\n",
        "\n",
        "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "    w = np.zeros_like(dim)\n",
        "    b = 0\n",
        "    return w,b  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506d7727-5593-4d93-cd84-05b6a87022f2"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',str(b))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df6484e-1f0a-4902-b2cd-0bd7c85d0559"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6"
      },
      "source": [
        "#import math\n",
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    #https://www.educative.io/edpresso/calculating-the-exponential-value-in-python\n",
        "    #sigmoid_z = 1/(1+math.exp(-z))\n",
        "    #https://numpy.org/doc/stable/reference/generated/numpy.exp.html\n",
        "    #we can also use math library to calculate the exponential value\n",
        "    sigmoid_z = 1/(1+np.exp(-z))\n",
        "    return sigmoid_z"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bfd856-21f9-4ea3-bbdb-3422caa66cff"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6"
      },
      "source": [
        "def logloss(y_true,y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    loss = 0\n",
        "    n = len(y_true)\n",
        "    for i in range(n):\n",
        "        # we can also use either math.log10 or math.log(value,10)\n",
        "        #loss += y_true[i]*math.log10(y_pred[i]) + (1-y_true[i])*math.log10(1-y_pred[i])\n",
        "        #https://www.geeksforgeeks.org/numpy-log10-python/#:~:text=About%20%3A,all%20the%20input%20array%20elements.\n",
        "        loss += y_true[i]*np.log10(y_pred[i]) + (1-y_true[i])*np.log10(1-y_pred[i])\n",
        "    loss = -1 * (loss/n)  \n",
        "\n",
        "    return loss"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819951a4-f943-4c85-e4b9-decea6a3b639"
      },
      "source": [
        "def grader_logloss(true,pred):\n",
        "  loss=logloss(true,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true,pred)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    #https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
        "    dw = x * (y - sigmoid(np.dot(w,x)+b)) - (alpha/N)*w\n",
        "    return dw"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c919aed2-9679-44f2-e9da-2f1cb3ef2a2e"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  #print(type(grad_dw))\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     '''In this function, we will compute gradient w.r.to b '''\n",
        "     db = y - sigmoid(np.dot(w,x) + b)\n",
        "     return db"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e20017-24e9-462b-daa8-692c6b074a85"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  #print(type(grad_db))\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAdc5ejEZ25"
      },
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
        "    ''' In this function, we will implement logistic regression'''   \n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    w,b = initialize_weights(X_train[0])\n",
        "    for i in range(0,epochs):\n",
        "        #https://stackoverflow.com/questions/49783594/for-loop-and-zip-in-python\n",
        "        #zip allows you to iterate two lists at the same time\n",
        "        for x,y in zip(X_train,y_train):\n",
        "              grad_dw = gradient_dw(x,y,w,b,alpha,N)\n",
        "              grad_db = gradient_db(x,y,w,b)\n",
        "              w = w + (eta0*grad_dw)\n",
        "              b = b + (eta0*grad_db)\n",
        "\n",
        "        y_train_pred = []      \n",
        "        for x_test in X_train:            \n",
        "             y_train_pred.append(sigmoid(np.dot(w,x)+b))\n",
        "        #computing and storing all the train loss values in a list\n",
        "        train_loss.append(logloss(y_train,y_train_pred)) \n",
        "\n",
        "        y_test_pred = []\n",
        "        for x in X_test:\n",
        "            y_test_pred.append(sigmoid(np.dot(w,x)+b))\n",
        "        #computing and storing all the test loss values in a list\n",
        "        test_loss.append(logloss(y_test,y_test_pred))\n",
        "\n",
        "        #https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points\n",
        "        #limiting the train and test loss upto 5 decimal places\n",
        "        print(f\"EPOCH: {i} Train Loss: {logloss(y_train, y_train_pred):.5f} Test Loss: {logloss(y_test, y_test_pred):.5f}\")\n",
        "\n",
        "\n",
        "        for i,loss in enumerate(train_loss):\n",
        "            \n",
        "            #print(i, loss)\n",
        "            if i > 0:\n",
        "                previous_loss = train_loss[i-1]\n",
        "                current_loss = loss\n",
        "                #print(previous_loss,current_loss)\n",
        "                if(previous_loss > current_loss) and (previous_loss - current_loss < 0.0001):\n",
        "                    break\n",
        "        else:\n",
        "            continue\n",
        "        break \n",
        "\n",
        "    return w,b,train_loss,test_loss"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3095c8b6-89b9-4c0e-a066-d8c10713319d"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(X_train)\n",
        "epochs=50\n",
        "w,b,loss_train,loss_test=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0 Train Loss: 0.29606 Test Loss: 0.17595\n",
            "EPOCH: 1 Train Loss: 0.29128 Test Loss: 0.16940\n",
            "EPOCH: 2 Train Loss: 0.29005 Test Loss: 0.16721\n",
            "EPOCH: 3 Train Loss: 0.28953 Test Loss: 0.16622\n",
            "EPOCH: 4 Train Loss: 0.28921 Test Loss: 0.16572\n",
            "EPOCH: 5 Train Loss: 0.28899 Test Loss: 0.16546\n",
            "EPOCH: 6 Train Loss: 0.28882 Test Loss: 0.16531\n",
            "EPOCH: 7 Train Loss: 0.28869 Test Loss: 0.16523\n",
            "EPOCH: 8 Train Loss: 0.28860 Test Loss: 0.16519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNjJMK00j-Mk",
        "outputId": "d85401fc-7c71-41f5-dee8-051a064a3355"
      },
      "source": [
        "print(w)\n",
        "print(b)\n",
        "#print(train_loss)\n",
        "#print(test_loss)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.42101199  0.19028065 -0.14496425  0.33810803 -0.20893349  0.56366252\n",
            " -0.44548271 -0.09228306  0.21670984  0.16844415  0.1940773   0.00318758\n",
            " -0.07669313  0.33872841  0.02189547]\n",
            "-0.8369332521112822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Zf_wPARlwY"
      },
      "source": [
        "<font color='red'>Goal of assignment</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eF_VSPSH2z"
      },
      "source": [
        "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx8Rs9rfEZ1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4a6184-5b54-4128-d899-4fb6d92ca1f2"
      },
      "source": [
        "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
        "w-clf.coef_, b-clf.intercept_"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.00235492,  0.00480499,  0.00362611, -0.00333604, -0.00074678,\n",
              "          0.00349673,  0.00694212,  0.00180507,  0.00743664, -0.01239711,\n",
              "         -0.00297461, -0.00103157,  0.00291056,  0.0002004 , -0.00077174]]),\n",
              " array([0.01620505]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230YbSgNSUrQ"
      },
      "source": [
        "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
        "\n",
        "* epoch number on X-axis\n",
        "* loss on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6GrRt7UeCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "37d22ffd-b78e-47ad-b8b9-54c36ec6a994"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(0,9)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_test, 'b', label='Test loss')\n",
        "plt.title('Training and Test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN8puFoEZtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ac6a03-261a-42f5-91e4-718930cc30d7"
      },
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95648\n",
            "0.95416\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}